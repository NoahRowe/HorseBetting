{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from helper import *\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "runs = pd.read_pickle(\"../Data/main_1.df\")\n",
    "\n",
    "#FEATURES = ['win_odds', 'horse_race_count', 'place_odds', 'best_odds', 'best_going_record', 'best_horse_record']\n",
    "FEATURES = ['horse_no', 'horse_rating', 'declared_weight', 'actual_weight', 'win_odds', 'draw', 'race_size', \n",
    "            'last_race_result', 'win_percent', 'avg_distance_time', 'going_type_record', 'horse_race_count', \n",
    "            'horse_record', 'surface_record', 'place_odds', 'weight_change_from_average', 'venue_change', \n",
    "            'venue_record', 'days_since_last_race', 'new_horse', 'best_odds', 'best_win_percent', \n",
    "            'best_going_record', 'best_horse_record', 'best_jockey_record', 'best_trainer_record', \n",
    "            'highest_actual_weight', 'lowest_actual_weight', 'start_speed', 'rode_before']\n",
    "\n",
    "TARGET = \"won\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAM TUNING STEPS:\n",
    "# find best booster type\n",
    "\n",
    "# set learning rate and find optimal n_estimators\n",
    "\n",
    "# Tune max_depth and min_child_weight\n",
    "\n",
    "# Tune gamma\n",
    "\n",
    "# Tune subsample and colsample_bytree\n",
    "\n",
    "# Regularization params\n",
    "\n",
    "# Lower learning rate and add more trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, grid, x, y, n_splits=4, n_jobs=4, verbose=2):\n",
    "    \n",
    "    cv = KFold(n_splits=n_splits, shuffle=True)\n",
    "    \n",
    "    gSearch = GridSearchCV(estimator=model, param_grid=grid, n_jobs=n_jobs, cv=cv, scoring=\"accuracy\", verbose=verbose)\n",
    "    \n",
    "    gSearch.fit(x, y)\n",
    "    \n",
    "    print(\"Best Params: \", gSearch.best_params_)\n",
    "    print(\"Score: {}\".format(gSearch.best_score_))\n",
    "    \n",
    "    return gSearch\n",
    "\n",
    "def testModel(search, x, y):\n",
    "    print(\"Determining Test Accuracy...\")\n",
    "    preds = search.predict(x)\n",
    "    score = accuracy_score(y, preds)\n",
    "    print(\"{:.2f}% accuracy\".format(score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline accuracy - 0.9183606753676548\n",
    "xgb_bl = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=500\n",
    ")\n",
    "grid_bl = {\n",
    "    \"silent\":[1]\n",
    "}\n",
    "#search_bl = trainModel(xgb_bl, grid_bl, runs[FEATURES], runs[TARGET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOOSTER TYPE\n",
    "xgb_1 = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=500\n",
    ")\n",
    "\n",
    "grid_1 = {\n",
    "    'booster':[\"gbtree\", \"gblinear\"]\n",
    "}\n",
    "#search_1 = trainModel(xgb_1, grid_1, runs[FEATURES], runs[TARGET])\n",
    "#best_booster=search_1.best_params_[\"booster\"]\n",
    "best_booster = 'gblinear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth and min_child_weight\n",
    "xgb_2_a = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=500,\n",
    "    booster=best_booster\n",
    ")\n",
    "\n",
    "grid_2_a = {\n",
    "    'max_depth':range(1,11)\n",
    "}\n",
    "#search_2_a = trainModel(xgb_2_a, grid_2_a, runs[FEATURES], runs[TARGET], verbose=5)\n",
    "#best_max_depth = search_2_a.best_params_[\"max_depth\"]\n",
    "best_max_depth = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth and min_child_weight\n",
    "xgb_2_b = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=500,\n",
    "    booster=best_booster,\n",
    "    max_depth = best_max_depth\n",
    ")\n",
    "\n",
    "grid_2_b = {\n",
    "    'min_child_weight':range(1,11)\n",
    "}\n",
    "#search_2_b = trainModel(xgb_2_b, grid_2_b, runs[FEATURES], runs[TARGET], verbose=5)\n",
    "#best_min_child_weight = search_2_b.best_params_[\"min_child_weight\"]\n",
    "best_min_child_weight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=4)]: Done  40 out of  40 | elapsed:  7.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params:  {'gamma': 0.0}\n",
      "Score: 0.9199214941081479\n"
     ]
    }
   ],
   "source": [
    "# gamma\n",
    "xgb_3 = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=500,\n",
    "    booster=best_booster,\n",
    "    max_depth=best_max_depth,\n",
    "    min_child_weight=best_min_child_weight,\n",
    ")\n",
    "\n",
    "grid_3 = {\n",
    "    'gamma':[i/1000. for i in range(0,10)]\n",
    "}\n",
    "search_3 = trainModel(xgb_3, grid_3, runs[FEATURES], runs[TARGET])\n",
    "best_gamma = search_3.best_params_[\"gamma\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=4)]: Done  64 out of  64 | elapsed: 12.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params:  {'colsample_bytree': 0.0, 'subsample': 0.0}\n",
      "Score: 0.9199340378626114\n"
     ]
    }
   ],
   "source": [
    "# subsample and colsample_bytree\n",
    "xgb_4 = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=500,\n",
    "    booster=best_booster,\n",
    "    max_depth=best_max_depth,\n",
    "    min_child_weight=best_min_child_weight,\n",
    "    gamma=best_gamma,\n",
    ")\n",
    "\n",
    "grid_4 = {\n",
    "    'subsample':[i/10.0 for i in range(0,4)],\n",
    "    'colsample_bytree':[i/10.0 for i in range(0,4)],\n",
    "}\n",
    "search_4 = trainModel(xgb_4, grid_4, runs[FEATURES], runs[TARGET])\n",
    "best_subsample = search_4.best_params_[\"subsample\"]\n",
    "best_colsample_bytree = search_4.best_params_[\"colsample_bytree\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:  4.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params:  {'reg_alpha': 0.001}\n",
      "Score: 0.9199466348518234\n"
     ]
    }
   ],
   "source": [
    "# Regularization params\n",
    "xgb_5 = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=500,\n",
    "    booster=best_booster,\n",
    "    max_depth=best_max_depth,\n",
    "    min_child_weight=best_min_child_weight,\n",
    "    gamma=best_gamma,\n",
    "    subsample=best_subsample,\n",
    "    colsample_bytree=best_colsample_bytree,\n",
    ")\n",
    "\n",
    "grid_5 = {\n",
    "    'reg_alpha':[1e-6,1e-5,1e-4,1e-3,1e-2]\n",
    "}\n",
    "search_5 = trainModel(xgb_5, grid_5, runs[FEATURES], runs[TARGET])\n",
    "best_reg_alpha = search_5.best_params_[\"reg_alpha\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gblinear 1 1 0.0 0.0 0.0 0.001\n",
      "CV 1/4\n",
      "CV 2/4\n",
      "CV 3/4\n",
      "CV 4/4\n",
      "Mean score: 0.298 +/- 0.015\n"
     ]
    }
   ],
   "source": [
    "# AFTER THESE ARE FIGURED OUT DECREASE LEARNING RATE AND INCREASE n_est\n",
    "# gblinear 1 1 0.0 0.0 0.0 0.001\n",
    "print(best_booster, best_max_depth, best_min_child_weight, best_gamma, best_subsample, best_colsample_bytree, \n",
    "      best_reg_alpha)     \n",
    "# Final model performance\n",
    "bestModel = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=500,\n",
    "    booster=best_booster,\n",
    "    max_depth=best_max_depth,\n",
    "    min_child_weight=best_min_child_weight,\n",
    "    gamma=best_gamma,\n",
    "    subsample=best_subsample,\n",
    "    colsample_bytree=best_colsample_bytree,\n",
    "    reg_alpha=best_reg_alpha\n",
    ")\n",
    "\n",
    "meanScore, stdScore = crossVal(runs, FEATURES, TARGET, bestModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.1, n_est: 1200\n",
      "CV 1/4\n",
      "CV 2/4\n",
      "CV 3/4\n",
      "CV 4/4\n",
      "Mean score: 0.298 +/- 0.011\n",
      "Learning Rate: 0.1, n_est: 1400\n",
      "CV 1/4\n",
      "CV 2/4\n",
      "CV 3/4\n",
      "CV 4/4\n",
      "Mean score: 0.298 +/- 0.008\n",
      "Learning Rate: 0.1, n_est: 1600\n",
      "CV 1/4\n",
      "CV 2/4\n",
      "CV 3/4\n",
      "CV 4/4\n",
      "Mean score: 0.298 +/- 0.005\n",
      "Learning Rate: 0.1, n_est: 1800\n",
      "CV 1/4\n",
      "CV 2/4\n",
      "CV 3/4\n",
      "CV 4/4\n",
      "Mean score: 0.298 +/- 0.010\n"
     ]
    }
   ],
   "source": [
    "#find best n_est for lower learning rate\n",
    "n_est = [1200,1400,1600,1800]\n",
    "lr = 0.1\n",
    "\n",
    "for i in range(len(n_est)):\n",
    "    \n",
    "    xgb_6 = XGBClassifier(\n",
    "        learning_rate =lr,\n",
    "        n_estimator = n_est[i],\n",
    "        booster=best_booster,\n",
    "        max_depth=best_max_depth,\n",
    "        min_child_weight=best_min_child_weight,\n",
    "        gamma=best_gamma,\n",
    "        subsample=best_subsample,\n",
    "        colsample_bytree=best_colsample_bytree,\n",
    "        reg_alpha=best_reg_alpha\n",
    "    )\n",
    "    \n",
    "    print(\"Learning Rate: {}, n_est: {}\".format(lr, n_est[i]))\n",
    "    meanScore, stdScore = crossVal(runs, FEATURES, TARGET, xgb_6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimator = 750,\n",
    "    booster='gblinear',\n",
    "    max_depth=1,\n",
    "    min_child_weight=1,\n",
    "    gamma=0.0,\n",
    "    subsample=0,\n",
    "    colsample_bytree=0,\n",
    "    reg_alpha=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gblinear', colsample_bylevel=None,\n",
       "              colsample_bynode=None, colsample_bytree=0.0, gamma=0.0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.1, max_delta_step=None, max_depth=1,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimator=1800, n_estimators=100, n_jobs=0,\n",
       "              num_parallel_tree=None, objective='binary:logistic',\n",
       "              random_state=0, reg_alpha=0.001, reg_lambda=0, scale_pos_weight=1,\n",
       "              subsample=0.0, tree_method=None, validate_parameters=False,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
